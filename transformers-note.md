# Transformers ã®ãƒ¡ãƒ¢

Hugging Face ğŸ¤— ã®ã€‚
LLM ã®ãƒãƒ¼ãƒˆã«æ›¸ã„ã¦ãŸã®ãŒã ã‚“ã ã‚“å¤§ãããªã‚Šã™ããŸã®ã§åˆ†ã‘ã‚‹ã€‚

## accelerate

ä¾¿åˆ©ã€‚PyTorch å°‚ç”¨?

- [accelerate Â· PyPI](https://pypi.org/project/accelerate/) - ã“ã®ã‚µãƒ³ãƒ—ãƒ«ãŒ
- [Using pipeline on large models with ğŸ¤— accelerate:](https://huggingface.co/docs/transformers/pipeline_tutorial#using-pipeline-on-large-models-with-accelerate) - ã“ã£ã¡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚‚
- [huggingface ã® accelerate ã‚’ä½¿ã£ã¦è¨“ç·´æ™‚ã® CUDA out of memory ã‚’å›é¿ã™ã‚‹ #è‡ªç„¶è¨€èªå‡¦ç† - Qiita](https://qiita.com/m__k/items/518ac10399c6c8753763)

## ãƒ¢ãƒ‡ãƒ«ã€ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

[Load pretrained instances with an AutoClass](https://huggingface.co/docs/transformers/autoclass_tutorial)

ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã¯ãƒ¢ãƒ‡ãƒ«ã®éª¨æ ¼ã®ã“ã¨ã§ã‚ã‚Šã€
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã¯ä¸ãˆã‚‰ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¯¾ã™ã‚‹é‡ã¿ã®ã“ã¨ã§ã‚ã‚‹ã€‚

ä¾‹ãˆã°ã€BERT ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šã€
bert-base-uncased ã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ã‚ã‚‹ã€‚

ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¾ãŸã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã©ã¡ã‚‰ã‹ã‚’æ„å‘³ã™ã‚‹ä¸€èˆ¬çš„ãªç”¨èªã§ã‚ã‚‹ã€‚

## PEFT

[PEFT](https://huggingface.co/docs/peft/index)

> PEFT(Parameter-Efficient Fine-Tuning)ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ãªãã€è¨“ç·´æ¸ˆã¿ã®è¨€èªãƒ¢ãƒ‡ãƒ«(PLM)ã‚’æ§˜ã€…ãªä¸‹æµã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«åŠ¹ç‡çš„ã«é©å¿œã•ã›ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚å¤§è¦æ¨¡ãª PLM ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ³•å¤–ãªã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€PEFT ãƒ¡ã‚½ãƒƒãƒ‰ã¯å°‘æ•°ã®(ä½™åˆ†ãª)ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚æœ€è¿‘ã®æœ€æ–°ã® PEFT æ‰‹æ³•ã¯ã€å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚
>
> PEFT ã¯ã€DeepSpeed ã¨ãƒ“ãƒƒã‚°ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’æ´»ç”¨ã—ãŸå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨ã® Transformers Accelerate ã¨ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚

[Load adapters with ğŸ¤— PEFT](https://huggingface.co/docs/transformers/peft)

> PEFT(Parameter-Efficient Fine Tuning)æ³•ã¯ã€fine-tuning ã®éš›ã«äº‹å‰ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‡çµã—ã€ãã®ä¸Šã«å°‘æ•°ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼)ã‚’è¿½åŠ ã™ã‚‹ã€‚ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®æƒ…å ±ã‚’å­¦ç¿’ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã•ã‚Œã‚‹ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€å®Œå…¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã™ã‚‹çµæœã‚’å‡ºã—ãªãŒã‚‰ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒéå¸¸ã«é«˜ãã€è¨ˆç®—é‡ãŒå°‘ãªã„ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚

Transformers ã® PEFT ãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€æ¨™æº–ã§ã¯

- [Low Rank Adapters (LoRA)](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

ãªã©ã€‚ãƒªã‚¹ãƒˆã¯ [PEFT - Supported methods](https://huggingface.co/docs/peft/index#supported-methods)

ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚è¿½åŠ å¯èƒ½(ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§)

ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯
[PEFT - Supported models](https://huggingface.co/docs/peft/index#supported-models)

å‚è€ƒè¨˜äº‹:

- [Load adapters with ğŸ¤— PEFT](https://huggingface.co/docs/transformers/peft)
- [Quicktour](https://huggingface.co/docs/peft/quicktour)
- [[ç¿»è¨³] Hugging face ã® PEFT ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ„ã‚¢ãƒ¼ #huggingface - Qiita](https://qiita.com/taka_yayoi/items/9196444274d6a63cda76)

ãŸã¨ãˆã°ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
[Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)ã®
Bert ã§åˆ†é¡ã®å ´åˆã¯
[ã“ã‚Œ](https://huggingface.co/docs/peft/index#sequence-classification)
ã‚’è¦‹ã¦ã€

task_type:

- "SEQ_CLS" - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã‚¯ãƒ©ã‚¹åˆ†é¡(Sequence Classification)ã‚¿ã‚¹ã‚¯
- "SEQ_2_SEQ_LM" - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é–“ã®å¤‰æ›(Sequence-to-Sequence Language Modeling)ã‚¿ã‚¹ã‚¯ã€‚ä¾‹ãˆã°æ–‡ç« ã®ç¿»è¨³ã€‚
- "CAUSAL_LM" - æœé–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°(Causal Language Modeling)ã‚¿ã‚¹ã‚¯
- "TOKEN_CLS" - ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®ã‚¯ãƒ©ã‚¹åˆ†é¡(Token Classification)ã‚¿ã‚¹ã‚¯ã€‚ä¾‹ãˆã°ã€å›ºæœ‰è¡¨ç¾æŠ½å‡º(NER)
- "QUESTION_ANS" - è³ªå•å¿œç­”(Question Answering)ã‚¿ã‚¹ã‚¯
- "FEATURE_EXTRACTION" - ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã®æŠ½å‡º(Feature Extraction)ã‚¿ã‚¹ã‚¯
